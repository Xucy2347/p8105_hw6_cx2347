p8105_hw6_cx2347
================
Chuyuan XU
2025-12-01

``` r
library(tidyverse)
library(stringr)
library(broom)
library(patchwork)
library(glmnet)
library(modelr)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.9,
  out.width = "90%"
)
```

### Problem 1

``` r
csv = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"

homicide_raw = read_csv(csv, na=c("NA", "", "."))
```

``` r
homicide_df = homicide_raw |>
  janitor::clean_names() |>
  
  # Create a city_state variable (e.g. “Baltimore, MD”)
  mutate(city_state = str_c(city, ', ', state)) |>
  
  # omit Dallas, TX; Phoenix, AZ; Kansas City, MO; Tulsa, AL
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")
  ) |>
  
  # limit the data among those for whom victim_race is white or black
  filter(
    victim_race %in% c("White", "Black")
  ) |>
  
  # change the victims' age into numbers
  # change the unknown victims' sex into NA
  # change the victim_race into a factor
  # create a indicator for resolved vs unresolved
  mutate(
    victim_age = if_else(victim_age == "Unknown", NA , victim_age),
    victim_age = as.numeric(victim_age),
    victim_sex =  if_else(victim_sex == "Unknown", NA , victim_sex),
    victim_race = fct_relevel(victim_race, "White"),
    resolved = as.numeric(disposition == "Closed by arrest")
  )
```

``` r
# fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors
fit_balt = homicide_df |>
  filter(city == "Baltimore") |>
  glm(
    resolved ~ victim_age + victim_race + victim_sex, 
    data = _,
    family = binomial()
  )

fit_balt_df = fit_balt |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    conf.low = exp(estimate - 1.96*std.error),
    conf.high = exp(estimate + 1.96*std.error),
  ) |>
  select(term, OR, conf.low, conf.high, p.value) 

fit_balt_df |> 
  mutate(
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |>
  nest(CI = c("conf.low", "conf.high")) |>
  select(term, OR, CI, p.value) |>
  knitr::kable(
    digits = 3,
    col.names = c("terms", "est OR", "95% CI (OR)", "P-value"),
    align = 'r'
  )
```

|            terms | est OR |  95% CI (OR) | P-value |
|-----------------:|-------:|-------------:|--------:|
|      (Intercept) |  3.164 | 1.989, 5.031 |   0.000 |
|       victim_age |  0.993 | 0.987, 1.000 |   0.043 |
| victim_raceBlack |  0.431 | 0.306, 0.607 |   0.000 |
|   victim_sexMale |  0.426 | 0.325, 0.558 |   0.000 |

The estimated adjusted odds ratio for solving homicides comparing male
victims to female victims keeping all other variables fixed is 0.426,
with a 95% confidence interval of (0.325, 0.558).

``` r
glm_cities_df = homicide_df |>
  nest(data = -city_state) |>
  mutate(
    models = map(data, \(df) glm(resolved ~ victim_age + victim_race + victim_sex, data = df)),
    results = map(models, broom::tidy)
  ) |>
  select(-data, -models) |> 
  unnest(results) |>
  
  # keep the results for only adj OR males over females
  filter(term == "victim_sexMale") |>
  
  mutate(
    OR = exp(estimate),
    conf.low = exp(estimate - 1.96*std.error),
    conf.high = exp(estimate + 1.96*std.error)
  ) |> 
  select(city_state, OR, conf.low, conf.high, p.value) |>
  
  # Organize cities according to estimated OR
  mutate(
    city_state = fct_reorder(city_state, OR)
  ) |>
  arrange(city_state)
```

``` r
# Create a plot that shows the estimated ORs and CIs for each city.
glm_cities_df |>
  mutate(
    fill = (OR > 1)
  ) |>
  ggplot(aes(y = city_state, x = OR, color = fill)) +
  geom_point() +
  geom_errorbar(
    aes(xmin = conf.low, xmax = conf.high),
    width = 0.2,
    alpha = 0.8
  ) +
  geom_vline(
    aes(xintercept = 1.0), 
    color = "grey",
    alpha = 0.8
  ) +
  theme_minimal() +
  labs(
    y = 'City, State',
    x = 'the Estimated OR with 95% CI',
    color = 'OR is greater than 1.0',
    title = 'the Adjusted Odds Ratio (and 95% CI) for Solving Homicides',
    subtitle = 'Comparing Male Victims to Female Victims'
  )
```

<img src="p8105_hw6_cx2347_files/figure-gfm/p1 plot-1.png" width="90%" />

#### Comments:

6 of 47 cities in the study have an OR greater than 1, for solving
homicides comparing male and female victims. In most cities, the odds
for solving homicides with a male victims is lower than that for solving
homicides with a female victims, adjusting for their age and race (white
or black). However, many 95% CI of the adjusted OR include the null
value of 1, we cannot conclude those associations are significant.

### Problem 2

``` r
data("weather_df")
```

``` r
weather_bs = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df))
  )

# get the r_sqr
weather_rsq = weather_bs |>
  mutate(
    results = map(models, broom::glance)
  ) |>
  unnest(results) |>
  mutate(
    id = as.numeric(.id)
  ) |>
  select(id = id, r.squared)

# get the beta_1/beta_2
weather_betas = weather_bs |>
  mutate(
    results = map(models, broom::tidy)
  ) |>
  select(.id, results) |>
  unnest(results) |>
  select(.id, term, estimate) |>
  pivot_wider(
    names_from = "term",
    values_from = "estimate",
    id_cols = .id
  ) |>
  mutate(
    beta.frac = tmin/prcp,
    id = as.numeric(.id)
  ) |>
  select(id, beta.frac)

# merge the result into one
weather_bs_rslt = full_join(weather_rsq, weather_betas, by = "id")
```

``` r
rsqr.dist = weather_bs_rslt |>
  ggplot(aes(x = r.squared)) +
  geom_density(fill = 'grey', alpha = 0.4) +
  geom_vline(aes(xintercept = mean(r.squared)), color = 'red', linetype = "dashed", alpha = 0.5) +
  geom_vline(aes(xintercept = median(r.squared)), color = 'blue', linetype = "solid", alpha = 0.8) +
  labs(
    x = "R sqaured"
  ) +
  theme_minimal()

beta.frac_dist = weather_bs_rslt |>
  ggplot(aes(x = beta.frac)) +
  geom_density(fill = 'grey', alpha = 0.4) +
  geom_vline(aes(xintercept = mean(beta.frac)), color = 'red', linetype = "dashed", alpha = 0.5) +
  geom_vline(aes(xintercept = median(beta.frac)), color = 'blue', linetype = "solid", alpha = 0.8) +
  labs(
    x = "Beta_tmin/Beta_prcp",
    y = ""
  ) +
  theme_minimal()

rsqr.dist + beta.frac_dist +
  plot_annotation(
    title = 'the Distribution of the Estimates',
    caption = 'the red dashed line is the mean and the blue solid line is the median')
```

<img src="p8105_hw6_cx2347_files/figure-gfm/p2 dist plot-1.png" width="90%" />

The above figure displays the distribution of Estimated
Beta_tmin/Beta_prcp and R-squared, where the red dashed line is the mean
and the blue solid line is the median. The r-squared is approximately
symmetrically distributed with a range of 0.930 and 0.955. The
Beta_tmin/Beta_prcp r-squared has a left-skewed distribution, with a
range between -450 and -100.

``` r
q = weather_bs_rslt |>
  pivot_longer(
    cols = r.squared:beta.frac,
    values_to = "estimate",
    names_to = "terms"
  ) |>
  group_by(terms) |>
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975)
  ) |>
  mutate(
    terms = case_match(
      terms,
      "beta.frac" ~ "est(Beta_tmin/Beta_prcp)",
      "r.squared" ~ "R-squared"
      )
    )

q |>
  knitr::kable(
    digits = 3,
    col.names = c("Estimates", "2.5% quantiles", "97.5% quantiles"),
    caption = "the 95% Confidence Intervals for Estimates"
  )
```

| Estimates                | 2.5% quantiles | 97.5% quantiles |
|:-------------------------|---------------:|----------------:|
| est(Beta_tmin/Beta_prcp) |       -273.249 |        -125.167 |
| R-squared                |          0.934 |           0.947 |

the 95% Confidence Intervals for Estimates

The above table shows the 95% Confidence Intervals for the Estimated
Beta_tmin/Beta_prcp and R-squared.

### Problem 3

``` r
bwt_raw = read_csv("data/birthweight.csv", na = c("NA", ".", ""))
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bwt_df = bwt_raw |>
    mutate(
      babysex = 
          case_match(babysex,
              1 ~ "male",
              2 ~ "female"
          ),
      
      babysex = fct_infreq(babysex),
      
      frace = 
          case_match(frace,
              1 ~ "white",
              2 ~ "black", 
              3 ~ "asian", 
              4 ~ "puerto rican", 
              8 ~ "other"),
      frace = fct_infreq(frace),
    
      mrace = 
          case_match(mrace,
              1 ~ "white",
              2 ~ "black", 
              3 ~ "asian", 
              4 ~ "puerto rican",
              8 ~ "other"),
      mrace = fct_infreq(mrace),
      
      malform = as.logical(malform)
    ) 

bwt_na = bwt_df |>
  filter(if_any(everything(), is.na)) |>
  summarise(
    across(
      everything(),
      ~ sum(is.na(.))
    )
  ) |>
  pivot_longer(
    cols = c("babysex":"wtgain"),
    values_to = "na_counts",
    names_to = "var_names"
  )
```

There are 4342 observations in the weightbirth raw dataset (`bwt_raw`)
with 20 variables.  
Categorical variables, including baby’s sex (`babysex`), the presence of
malformations that could affect weight (`malform`), father’s and
mother’s race (`frace` and `mrace`) were converted from numeric to
factor prior to the analysis. Father’s and mother’s race as “unknown”
(category 9) will be treated as NA in the later analysis.  
After tidying the data, there are 0 observations with missing values.

``` r
# select model with Lasso
x = model.matrix(bwt ~ ., bwt_df)[,-1]
y = bwt_df |> pull(bwt)
lambda = 10^(seq(-2, 2.75, 0.1))

lasso_fit =
  glmnet(x, y, lambda = lambda)

lasso_cv =
  cv.glmnet(x, y, lambda = lambda)

lambda_opt = lasso_cv[["lambda.min"]]

lasso_fit_opt =  
  glmnet(x, y, lambda = lambda_opt) |> 
  broom::tidy()
```

A Lasso Regression model is performed to select the most parsimonious
model with the smallest tuning parameter lambda. The optimized model has
a lambda value of 1.585 and kept babysexfemale, bhead, blength, delwt,
fincome, fracepuerto rican, gaweeks, menarche, mheight, momage,
mraceblack, mracepuerto rican, mraceasian, parity, smoken, wtgain in the
linear regression model. However, in the lasso, dummy variables for
mrace and frace are separated. In the proposed model, all the frace and
mrace categories are dropped.

``` r
# in the lasso, categorical variables for mrace and frace are separated.
# reintroduce all the frace and mrace categories.
fit_new = 
  lm(
    bwt ~ 
      bhead + blength + delwt + fincome + gaweeks + menarche + 
      mheight + momage + parity + smoken + wtgain, 
    data = bwt_df)

fit_new |>
  broom::tidy() |>
  knitr::kable(
    digits = 3,
    caption = "Summary of the new fitted Linear Regression Model Estimating Baby Birthweight"
  )
```

| term        |  estimate | std.error | statistic | p.value |
|:------------|----------:|----------:|----------:|--------:|
| (Intercept) | -6522.119 |   136.512 |   -47.777 |   0.000 |
| bhead       |   132.407 |     3.465 |    38.217 |   0.000 |
| blength     |    77.340 |     2.054 |    37.646 |   0.000 |
| delwt       |     1.049 |     0.238 |     4.408 |   0.000 |
| fincome     |     1.012 |     0.175 |     5.798 |   0.000 |
| gaweeks     |    13.312 |     1.486 |     8.960 |   0.000 |
| menarche    |    -5.734 |     2.947 |    -1.946 |   0.052 |
| mheight     |     8.491 |     1.801 |     4.715 |   0.000 |
| momage      |     4.487 |     1.197 |     3.748 |   0.000 |
| parity      |    97.934 |    41.316 |     2.370 |   0.018 |
| smoken      |    -2.888 |     0.578 |    -5.000 |   0.000 |
| wtgain      |     2.910 |     0.441 |     6.600 |   0.000 |

Summary of the new fitted Linear Regression Model Estimating Baby
Birthweight

``` r
fit_new |>
  broom::glance() |>
  select(r.squared, statistic, p.value) |>
  knitr::kable(
    digits = 3,
    caption = "Performance Summary"
  )
```

| r.squared | statistic | p.value |
|----------:|----------:|--------:|
|     0.705 |   942.749 |       0 |

Performance Summary

The above table displays the estimate coefficient of the predictors in
the linear regression model estimating baby birth weights. All of the
estimates shows either significant or approximately significant
association with the baby birthweight. The overall model is significant
with a p-value \< 0.05, and 70.5% variance in the birthweight can be
explained by the model.

``` r
# plot of model residuals against fitted values
bwt_df |>
  modelr::add_residuals(fit_new) |>
  modelr::add_predictions(fit_new) |>
  
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal() +
  labs(
    x = "Fitted Values",
    y = "Model Residuals",
    title = "Model Residuals against Fitted Values"
  )
```

    ## `geom_smooth()` using formula = 'y ~ x'

<img src="p8105_hw6_cx2347_files/figure-gfm/p3_residuals vs. fitted values-1.png" width="90%" />

``` r
train_df = 
  crossv_mc(bwt_df, 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = train_df|>
  mutate(
    # model 1 using length at birth and gestational age as predictors (main effects only)
    main_mod = 
      map(train, 
          \(df) 
          lm(
            bwt ~ 
              blength + gaweeks, 
            data = df)
          ),
    
    # model 2 using head circumference, length, sex, and all interactions (including the three-way interaction) between these
    inter_mod  = 
      map(train, 
          \(df) 
          lm(
            bwt ~ 
              bhead + blength + babysex + 
              bhead * blength + babysex * bhead + blength * babysex + 
              bhead * blength * babysex,
            data = df)
          ),
    
    # model 3: model proposed above
    my_mod  = 
      map(train, 
          \(df) 
            lm(
              bwt ~ 
                bhead + blength + delwt + fincome + gaweeks + menarche + 
                mheight + momage + parity + smoken + wtgain, 
              data = df)
            )
    ) |> 
  
  mutate(
    rmse_main = map2_dbl(main_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_inter = map2_dbl(inter_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_my = map2_dbl(my_mod, test, \(mod, df) rmse(model = mod, data = df))
    )
```

    ## Warning: There were 2 warnings in `mutate()`.
    ## The first warning was:
    ## ℹ In argument: `rmse_my = map2_dbl(my_mod, test, function(mod, df) rmse(model =
    ##   mod, data = df))`.
    ## Caused by warning in `predict.lm()`:
    ## ! prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
    ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.

``` r
# plot the prediction error distribution for each candidate model
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |>
  
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin(alpha = 0.5) +
  theme_minimal() +
  labs(
    fill = "Model"
  )
```

<img src="p8105_hw6_cx2347_files/figure-gfm/p3_plot rmse-1.png" width="90%" />

Across all three models, the model from Lasso selection has the lowest
rmse and is, therefore, more preferred.
